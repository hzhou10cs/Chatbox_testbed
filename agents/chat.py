# llm_agents.py
"""
High-level LLM agents for the health assistant:

- ChatAgent: main conversational health coach.
- ExtractorAgent: summarizes recent conversation and extracts key info
  (for goals, obstacles, progress, etc.), to be stored in JSON.

Both use an OpenAI-style /v1/chat/completions endpoint, typically served
by vLLM or another compatible backend.
"""

from __future__ import annotations

from typing import Any, Dict, List

from agents.base import OpenAIStyleClient

from llm_config import (
    UI_TEST_MODE,
    VLLM_BASE_URL,
    CHAT_MODEL_NAME,
)

from .prompt_chat import COACH_SYSTEM_PROMPT_V1, COACH_SYSTEM_PROMPT_FOR1B

class ChatAgent:
    """
    Health coach chat agent.

    This is the main "talking" brain of your web app. It takes:
    - current user input
    - the logged-in user state
    - user profile state
    - latest goals feedback

    and returns a reply.
    """

    def __init__(self):
        self.client = OpenAIStyleClient(VLLM_BASE_URL, CHAT_MODEL_NAME)

    def _build_system_prompt(
        self, 
        user_state: dict,
        user_info_state: dict | None, 
        addition_progress: str
        ) -> str:
        base_system_prompt = COACH_SYSTEM_PROMPT_FOR1B

        context_parts: List[str] = []

        # User profile summary (if available)
        if user_info_state:
            username = user_state.get("username") or "user"
            first_name = user_info_state.get("first_name") or username
            last_name = user_info_state.get("last_name") or ""
            full_name = (first_name + " " + last_name).strip()

            gender = user_info_state.get("gender") or "unknown gender"
            height = user_info_state.get("height") or "unknown height"
            initial_weight = user_info_state.get("initial_weight") or "unknown weight"
            statement = user_info_state.get("weight_loss_statement") or ""
            health_history = user_info_state.get("health_history") or ""

            profile_text = [
                "User profile summary:",
                f"- Name: {full_name}",
                f"- Gender: {gender}",
                f"- Height: {height}",
                f"- Initial weight: {initial_weight}",
            ]
            if statement:
                profile_text.append(f"- Weight loss statement: {statement}")
            if health_history:
                profile_text.append(f"- Health history: {health_history}")

            context_parts.append("\n".join(profile_text))

        # Latest addition progress, if any
        if addition_progress:
            context_parts.append(addition_progress)

        system_content = base_system_prompt
        if context_parts:
            system_content += "\n\nAdditional context:\n" + "\n\n".join(context_parts)

        return system_content
    
    def build_system_prompt_for_ui(
        self,
        user_state: dict,
        user_info_state: dict | None,
        addition_progress: str,
    ) -> str:
        """Return the full system prompt that will be sent to the model."""
        return self._build_system_prompt(user_state, user_info_state, addition_progress)

    def build_messages(
        self,
        user_input: str,
        user_state: dict,
        user_info_state: dict | None,
        addition_progress: str,
    ) -> List[Dict[str, str]]:
        system_content = self._build_system_prompt(user_state, user_info_state, addition_progress)
        messages: List[Dict[str, str]] = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_input},
        ]
        return messages

    def reply(
        self,
        user_input: str,
        user_state: dict,
        user_info_state: dict | None,
        addition_progress: str,
    ) -> str:
        """Main entry point used by business logic."""
        username = user_state.get("username") or "user"

        if UI_TEST_MODE:
            return (
                "[UI test dummy reply]\n"
                f"User: {username}\n"
                f"You said: {user_input}\n\n"
                "This is a placeholder response. In production, this will be "
                "generated by a local LLM served by vLLM."
            )

        messages = self.build_messages(user_input, user_state, user_info_state, addition_progress)

        try:
            reply_text = self.client.chat(messages)
            return reply_text
        except Exception as e:
            return (
                "[LLM error]\n"
                f"Failed to get reply from chat agent: {e}\n\n"
                "Please check that the LLM server is running and the configuration "
                "in llm_config.py / environment variables is correct."
            )

# Module-level singletons, so app.py and logic modules can just import them.
chat_agent = ChatAgent()
