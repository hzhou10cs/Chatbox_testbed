# llm_agents.py
"""
High-level LLM agents for the health assistant:

- ChatAgent: main conversational health coach.
- ExtractorAgent: summarizes recent conversation and extracts key info
  (for goals, obstacles, progress, etc.), to be stored in JSON.

Both use an OpenAI-style /v1/chat/completions endpoint, typically served
by vLLM or another compatible backend.
"""

from __future__ import annotations

from typing import Any, Dict, List

from agents.base import OpenAIStyleClient

from llm_config import (
    UI_TEST_MODE,
    LLM_BASE_URL,
    CHAT_MODEL_NAME,
)

from .prompt_chat import COACH_SYSTEM_PROMPT_V1

class ChatAgent:
    """
    Health coach chat agent.

    This is the main "talking" brain of your web app. It takes:
    - current user input
    - the logged-in user state
    - user profile state
    - latest goals feedback

    and returns a reply.
    """

    def __init__(self):
        self.client = OpenAIStyleClient(LLM_BASE_URL, CHAT_MODEL_NAME)

    def _build_system_prompt(
        self, 
        user_state: dict,
        user_info_state: dict | None, 
        addition_progress: str,
        prompt_patch: str | None = None,
        base_prompt: str | None = None,
        ) -> str:
        system_content = base_prompt or COACH_SYSTEM_PROMPT_V1
        if prompt_patch:
            system_content += "\n\nPrompt patch:\n" + prompt_patch.strip()

        return system_content
    
    def build_system_prompt_for_ui(
        self,
        user_state: dict,
        user_info_state: dict | None,
        addition_progress: str,
        prompt_patch: str | None = None,
        base_prompt: str | None = None,
    ) -> str:
        """Return the full system prompt that will be sent to the model."""
        return self._build_system_prompt(
            user_state,
            user_info_state,
            addition_progress,
            prompt_patch=prompt_patch,
            base_prompt=base_prompt,
        )

    def build_messages(
        self,
        user_input: str,
        user_state: dict,
        user_info_state: dict | None,
        addition_progress: str,
        prompt_patch: str | None = None,
        base_prompt: str | None = None,
    ) -> List[Dict[str, str]]:
        system_content = self._build_system_prompt(
            user_state,
            user_info_state,
            addition_progress,
            prompt_patch=prompt_patch,
            base_prompt=base_prompt,
        )
        messages: List[Dict[str, str]] = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_input},
        ]
        return messages

    def reply(
        self,
        user_input: str,
        user_state: dict,
        user_info_state: dict | None,
        addition_progress: str,
        prompt_patch: str | None = None,
        base_prompt: str | None = None,
    ) -> str:
        """Main entry point used by business logic."""
        username = user_state.get("username") or "user"

        if UI_TEST_MODE:
            return (
                "[UI test dummy reply]\n"
                f"User: {username}\n"
                f"You said: {user_input}\n\n"
                "This is a placeholder response. In production, this will be "
                "generated by a local LLM served by vLLM."
            )

        messages = self.build_messages(
            user_input,
            user_state,
            user_info_state,
            addition_progress,
            prompt_patch=prompt_patch,
            base_prompt=base_prompt,
        )

        try:
            reply_text = self.client.chat(messages)
            return reply_text
        except Exception as e:
            return (
                "[LLM error]\n"
                f"Failed to get reply from chat agent: {e}\n\n"
                "Please check that the LLM provider is running and the configuration "
                "in llm_config.py / environment variables is correct."
            )

# Module-level singletons, so app.py and logic modules can just import them.
chat_agent = ChatAgent()
