# llm_agents.py
"""
High-level LLM agents for the health assistant:

- ChatAgent: main conversational health coach.
- ExtractorAgent: summarizes recent conversation and extracts key info
  (for goals, obstacles, progress, etc.), to be stored in JSON.

Both use an OpenAI-style /v1/chat/completions endpoint, typically served
by vLLM or another compatible backend.
"""

from __future__ import annotations

from typing import Any, Dict, List

from agents.base import OpenAIStyleClient

from llm_config import (
    UI_TEST_MODE,
    LLM_BASE_URL,
    CHAT_MODEL_NAME,
)

from .prompt_chat import COACH_SYSTEM_PROMPT_FEWSHOT, COACH_SYSTEM_PROMPT_V1

class ChatAgent:
    """
    Health coach chat agent.

    This is the main "talking" brain of your web app. It takes:
    - current user input
    - the logged-in user state
    - user profile state
    - latest goals feedback

    and returns a reply.
    """

    def __init__(self):
        self.client = OpenAIStyleClient(LLM_BASE_URL, CHAT_MODEL_NAME)

    def _build_system_messages(
        self,
        prompt_patch: str | None,
        base_prompt: str | None,
        include_fewshot: bool,
    ) -> List[Dict[str, str]]:
        messages: List[Dict[str, str]] = []
        base_content = (base_prompt or COACH_SYSTEM_PROMPT_V1).strip()
        if base_content:
            messages.append({"role": "system", "content": base_content})
        if prompt_patch:
            messages.append({"role": "system", "content": prompt_patch.strip()})
        if include_fewshot and COACH_SYSTEM_PROMPT_FEWSHOT.strip():
            messages.append({"role": "system", "content": COACH_SYSTEM_PROMPT_FEWSHOT.strip()})
        return messages
    
    def build_system_prompt_for_ui(
        self,
        user_state: dict,
        user_info_state: dict | None,
        addition_progress: str,
        prompt_patch: str | None = None,
        base_prompt: str | None = None,
        include_fewshot: bool = True,
    ) -> str:
        """Return the full system prompt that will be sent to the model."""
        system_messages = self._build_system_messages(
            prompt_patch=prompt_patch,
            base_prompt=base_prompt,
            include_fewshot=include_fewshot,
        )
        return "\n\n---\n\n".join(m["content"] for m in system_messages)

    def build_messages(
        self,
        user_input: str,
        user_state: dict,
        user_info_state: dict | None,
        addition_progress: str,
        prompt_patch: str | None = None,
        base_prompt: str | None = None,
        memory_text: str | None = None,
        recent_history_text: str | None = None,
        include_fewshot: bool = True,
    ) -> List[Dict[str, str]]:
        messages = self._build_system_messages(
            prompt_patch=prompt_patch,
            base_prompt=base_prompt,
            include_fewshot=include_fewshot,
        )
        if memory_text:
            messages.append({"role": "user", "content": memory_text})
        if recent_history_text:
            messages.append({"role": "user", "content": recent_history_text})
        if user_input.strip():
            messages.append({"role": "user", "content": user_input})
        return messages

    def reply(
        self,
        user_input: str,
        user_state: dict,
        user_info_state: dict | None,
        addition_progress: str,
        prompt_patch: str | None = None,
        base_prompt: str | None = None,
        memory_text: str | None = None,
        recent_history_text: str | None = None,
        include_fewshot: bool = True,
    ) -> str:
        """Main entry point used by business logic."""
        username = user_state.get("username") or "user"

        if UI_TEST_MODE:
            return (
                "[UI test dummy reply]\n"
                f"User: {username}\n"
                f"You said: {user_input}\n\n"
                "This is a placeholder response. In production, this will be "
                "generated by a local LLM served by vLLM."
            )

        messages = self.build_messages(
            user_input,
            user_state,
            user_info_state,
            addition_progress,
            prompt_patch=prompt_patch,
            base_prompt=base_prompt,
            memory_text=memory_text,
            recent_history_text=recent_history_text,
            include_fewshot=include_fewshot,
        )

        try:
            reply_text = self.client.chat(messages)
            return reply_text
        except Exception as e:
            return (
                "[LLM error]\n"
                f"Failed to get reply from chat agent: {e}\n\n"
                "Please check that the LLM provider is running and the configuration "
                "in llm_config.py / environment variables is correct."
            )

# Module-level singletons, so app.py and logic modules can just import them.
chat_agent = ChatAgent()
